{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from bs4 import BeautifulSoup as bs\nfrom bs4.element import Tag\nimport codecs\n\n# Read data file and parse the XML\nwith codecs.open(\"reuters.xml\", \"r\", \"utf-8\") as infile:\n    soup = bs(infile, \"html5lib\")\n\ndocs = []\nfor elem in soup.find_all(\"document\"):\n    texts = []\n\n    # Loop through each child of the element under \"textwithnamedentities\"\n    for c in elem.find(\"textwithnamedentities\").children:\n        if type(c) == Tag:\n            if c.name == \"namedentityintext\":\n                label = \"N\"  # part of a named entity\n            else:\n                label = \"I\"  # irrelevant word\n            for w in c.text.split(\" \"):\n                if len(w) > 0:\n                    texts.append((w, label))\n    docs.append(texts)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "ename": "<class 'FileNotFoundError'>",
          "evalue": "[Errno 44] No such file or directory: 'reuters.xml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcodecs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Read data file and parse the XML\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreuters.xml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m      7\u001b[0m     soup \u001b[38;5;241m=\u001b[39m bs(infile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/lib/python3.10/codecs.py:905\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    902\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Force opening of the file in binary mode\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     mode \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 905\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 44] No such file or directory: 'reuters.xml'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "docs[0][:10]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'docs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdocs\u001b[49m[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import nltk\ndata = []\nfor i, doc in enumerate(docs):\n\n    # Obtain the list of tokens in the document\n    tokens = [t for t, label in doc]\n\n    # Perform POS tagging\n    tagged = nltk.pos_tag(tokens)\n\n    # Take the word, POS tag, and its label\n    data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'docs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdocs\u001b[49m):\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Obtain the list of tokens in the document\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t, label \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Perform POS tagging\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "data[0]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "ename": "<class 'IndexError'>",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "def word2features(doc, i):\n    word = doc[i][0]\n    postag = doc[i][1]\n\n    # Common features for all words\n    features = [\n        'bias',\n        'word.lower=' + word.lower(),\n        'word[-3:]=' + word[-3:],\n        'word[-2:]=' + word[-2:],\n        'word.isupper=%s' % word.isupper(),\n        'word.istitle=%s' % word.istitle(),\n        'word.isdigit=%s' % word.isdigit(),\n        'postag=' + postag\n    ]\n\n    # Features for words that are not\n    # at the beginning of a document\n    if i > 0:\n        word1 = doc[i-1][0]\n        postag1 = doc[i-1][1]\n        features.extend([\n            '-1:word.lower=' + word1.lower(),\n            '-1:word.istitle=%s' % word1.istitle(),\n            '-1:word.isupper=%s' % word1.isupper(),\n            '-1:word.isdigit=%s' % word1.isdigit(),\n            '-1:postag=' + postag1\n        ])\n    else:\n        # Indicate that it is the 'beginning of a document'\n        features.append('BOS')\n\n    # Features for words that are not\n    # at the end of a document\n    if i < len(doc)-1:\n        word1 = doc[i+1][0]\n        postag1 = doc[i+1][1]\n        features.extend([\n            '+1:word.lower=' + word1.lower(),\n            '+1:word.istitle=%s' % word1.istitle(),\n            '+1:word.isupper=%s' % word1.isupper(),\n            '+1:word.isdigit=%s' % word1.isdigit(),\n            '+1:postag=' + postag1\n        ])\n    else:\n        # Indicate that it is the 'end of a document'\n        features.append('EOS')\n\n    return features",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\n# A function for extracting features in documents\ndef extract_features(doc):\n    return [word2features(doc, i) for i in range(len(doc))]\n\n# A function fo generating the list of labels for each document\ndef get_labels(doc):\n    return [label for (token, postag, label) in doc]\n\nX = [extract_features(doc) for doc in data]\ny = [get_labels(doc) for doc in data]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pycrfsuite\ntrainer = pycrfsuite.Trainer(verbose=True)\n\n# Submit training data to the trainer\nfor xseq, yseq in zip(X_train, y_train):\n    trainer.append(xseq, yseq)\n\n# Set the parameters of the model\ntrainer.set_params({\n    # coefficient for L1 penalty\n    'c1': 0.1,\n\n    # coefficient for L2 penalty\n    'c2': 0.01,  \n\n    # maximum number of iterations\n    'max_iterations': 200,\n\n    # whether to include transitions that\n    # are possible, but not observed\n    'feature.possible_transitions': True\n})\n\n# Provide a file name as a parameter to the train function, such that\n# the model will be saved to the file when training is finished\ntrainer.train('crf.model')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "tagger = pycrfsuite.Tagger()\ntagger.open('crf.model')\ny_pred = [tagger.tag(xseq) for xseq in X_test]\n\n# Let's take a look at a random sample in the testing set\ni = 12\nfor x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n    print(\"%s (%s)\" % (y, x))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.metrics import classification_report\n\n# Create a mapping of labels to indices\nlabels = {\"N\": 1, \"I\": 0}\n\n# Convert the sequences of tags into a 1-dimensional array\npredictions = np.array([labels[tag] for row in y_pred for tag in row])\ntruths = np.array([labels[tag] for row in y_test for tag in row])\n\n# Print out the classification report\nprint(classification_report(\n    truths, predictions,\n    target_names=[\"I\", \"N\"]))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}